{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Perceptron From Scratch\n",
    "\n",
    "This notebook implements a single-neuron logistic model (perceptron) from scratch using pure NumPy to classify fruits (apples vs bananas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fruit dataset\n",
    "df = pd.read_csv('fruit.csv')\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(df.head(10))\n",
    "\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(\"0 = Apple, 1 = Banana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Feature distributions\n",
    "for i, feature in enumerate(['length_cm', 'weight_g', 'yellow_score']):\n",
    "    ax = axes[i//2, i%2]\n",
    "    for label in [0, 1]:\n",
    "        data = df[df['label'] == label][feature]\n",
    "        fruit_name = 'Apple' if label == 0 else 'Banana'\n",
    "        ax.hist(data, alpha=0.7, label=fruit_name, bins=8)\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'Distribution of {feature}')\n",
    "    ax.legend()\n",
    "\n",
    "# Scatter plot\n",
    "ax = axes[1, 1]\n",
    "colors = ['red', 'yellow']\n",
    "labels = ['Apple', 'Banana']\n",
    "for label in [0, 1]:\n",
    "    data = df[df['label'] == label]\n",
    "    ax.scatter(data['length_cm'], data['weight_g'], \n",
    "              c=colors[label], label=labels[label], alpha=0.7, s=60)\n",
    "ax.set_xlabel('Length (cm)')\n",
    "ax.set_ylabel('Weight (g)')\n",
    "ax.set_title('Length vs Weight')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perceptron Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.01, random_seed=42):\n",
    "        \"\"\"\n",
    "        Initialize perceptron with random weights and bias\n",
    "        \"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "        self.accuracy_history = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function with clipping to prevent overflow\"\"\"\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def initialize_parameters(self, n_features):\n",
    "        \"\"\"Initialize weights and bias randomly\"\"\"\n",
    "        self.weights = np.random.normal(0, 0.1, n_features)\n",
    "        self.bias = np.random.normal(0, 0.1)\n",
    "        print(f\"Initialized weights: {self.weights}\")\n",
    "        print(f\"Initialized bias: {self.bias}\")\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass: compute predictions\"\"\"\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "    def compute_cost(self, y_true, y_pred):\n",
    "        \"\"\"Compute binary cross-entropy loss\"\"\"\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        cost = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return cost\n",
    "    \n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        \"\"\"Compute accuracy\"\"\"\n",
    "        predictions = (y_pred >= 0.5).astype(int)\n",
    "        return np.mean(predictions == y_true)\n",
    "    \n",
    "    def fit(self, X, y, epochs=500, verbose=True):\n",
    "        \"\"\"Train the perceptron using batch gradient descent\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.initialize_parameters(n_features)\n",
    "        \n",
    "        # Store initial random predictions\n",
    "        initial_predictions = self.forward(X)\n",
    "        initial_cost = self.compute_cost(y, initial_predictions)\n",
    "        initial_accuracy = self.compute_accuracy(y, initial_predictions)\n",
    "        \n",
    "        print(f\"\\nInitial random predictions:\")\n",
    "        print(f\"Initial cost: {initial_cost:.4f}\")\n",
    "        print(f\"Initial accuracy: {initial_accuracy:.4f}\")\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute cost and accuracy\n",
    "            cost = self.compute_cost(y, y_pred)\n",
    "            accuracy = self.compute_accuracy(y, y_pred)\n",
    "            \n",
    "            # Store metrics\n",
    "            self.cost_history.append(cost)\n",
    "            self.accuracy_history.append(accuracy)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = np.dot(X.T, (y_pred - y)) / n_samples\n",
    "            db = np.mean(y_pred - y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs} - Cost: {cost:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if cost < 0.05:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} - Target cost reached: {cost:.4f}\")\n",
    "                break\n",
    "        \n",
    "        # Final results\n",
    "        final_predictions = self.forward(X)\n",
    "        final_cost = self.compute_cost(y, final_predictions)\n",
    "        final_accuracy = self.compute_accuracy(y, final_predictions)\n",
    "        \n",
    "        print(f\"\\nTraining completed!\")\n",
    "        print(f\"Final cost: {final_cost:.4f}\")\n",
    "        print(f\"Final accuracy: {final_accuracy:.4f}\")\n",
    "        print(f\"Final weights: {self.weights}\")\n",
    "        print(f\"Final bias: {self.bias}\")\n",
    "        \n",
    "        return {\n",
    "            'initial_cost': initial_cost,\n",
    "            'initial_accuracy': initial_accuracy,\n",
    "            'final_cost': final_cost,\n",
    "            'final_accuracy': final_accuracy,\n",
    "            'epochs_trained': len(self.cost_history)\n",
    "        }\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        probabilities = self.forward(X)\n",
    "        return (probabilities >= 0.5).astype(int)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return prediction probabilities\"\"\"\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "X = df[['length_cm', 'weight_g', 'yellow_score']].values\n",
    "y = df['label'].values\n",
    "\n",
    "print(\"Original features:\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Feature means: {np.mean(X, axis=0)}\")\n",
    "print(f\"Feature stds: {np.std(X, axis=0)}\")\n",
    "\n",
    "# Normalize features for better convergence\n",
    "X_normalized = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "print(\"\\nNormalized features:\")\n",
    "print(f\"Feature means: {np.mean(X_normalized, axis=0)}\")\n",
    "print(f\"Feature stds: {np.std(X_normalized, axis=0)}\")\n",
    "\n",
    "print(f\"\\nLabels: {y}\")\n",
    "print(f\"Label distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train perceptron\n",
    "perceptron = Perceptron(learning_rate=0.1, random_seed=42)\n",
    "results = perceptron.fit(X_normalized, y, epochs=500, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot cost\n",
    "ax1.plot(perceptron.cost_history, 'b-', linewidth=2, label='Training Loss')\n",
    "ax1.set_title('Training Loss Over Epochs', fontsize=14)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Binary Cross-Entropy Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(perceptron.accuracy_history, 'r-', linewidth=2, label='Training Accuracy')\n",
    "ax2.set_title('Training Accuracy Over Epochs', fontsize=14)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training completed in {len(perceptron.cost_history)} epochs\")\n",
    "print(f\"Final loss: {perceptron.cost_history[-1]:.4f}\")\n",
    "print(f\"Final accuracy: {perceptron.accuracy_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = perceptron.predict(X_normalized)\n",
    "probabilities = perceptron.predict_proba(X_normalized)\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = df.copy()\n",
    "results_df['probability'] = probabilities\n",
    "results_df['prediction'] = predictions\n",
    "results_df['correct'] = (predictions == y)\n",
    "results_df['fruit_name'] = results_df['label'].map({0: 'Apple', 1: 'Banana'})\n",
    "results_df['pred_name'] = results_df['prediction'].map({0: 'Apple', 1: 'Banana'})\n",
    "\n",
    "print(\"Prediction Results:\")\n",
    "display(results_df[['fruit_name', 'pred_name', 'probability', 'correct']])\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {np.mean(predictions == y):.4f}\")\n",
    "print(f\"Correct predictions: {np.sum(predictions == y)}/{len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Learning Rate Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 1.0]\n",
    "colors = ['blue', 'green', 'red', 'orange']\n",
    "results_comparison = {}\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    print(f\"\\nTesting learning rate: {lr}\")\n",
    "    perceptron_test = Perceptron(learning_rate=lr, random_seed=42)\n",
    "    test_results = perceptron_test.fit(X_normalized, y, epochs=500, verbose=False)\n",
    "    results_comparison[lr] = test_results\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(perceptron_test.cost_history, color=colors[i], label=f'LR={lr}', linewidth=2)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(perceptron_test.accuracy_history, color=colors[i], label=f'LR={lr}', linewidth=2)\n",
    "    \n",
    "    print(f\"Epochs trained: {test_results['epochs_trained']}\")\n",
    "    print(f\"Final accuracy: {test_results['final_accuracy']:.4f}\")\n",
    "\n",
    "# Configure loss plot\n",
    "ax1.set_title('Loss vs Epochs for Different Learning Rates', fontsize=14)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Configure accuracy plot\n",
    "ax2.set_title('Accuracy vs Epochs for Different Learning Rates', fontsize=14)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Convergence comparison\n",
    "lrs = list(results_comparison.keys())\n",
    "epochs_to_converge = [results_comparison[lr]['epochs_trained'] for lr in lrs]\n",
    "final_accuracies = [results_comparison[lr]['final_accuracy'] for lr in lrs]\n",
    "\n",
    "ax3.bar(range(len(lrs)), epochs_to_converge, color=colors)\n",
    "ax3.set_title('Epochs to Convergence', fontsize=14)\n",
    "ax3.set_xlabel('Learning Rate')\n",
    "ax3.set_ylabel('Epochs')\n",
    "ax3.set_xticks(range(len(lrs)))\n",
    "ax3.set_xticklabels(lrs)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "ax4.bar(range(len(lrs)), final_accuracies, color=colors)\n",
    "ax4.set_title('Final Accuracy', fontsize=14)\n",
    "ax4.set_xlabel('Learning Rate')\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.set_xticks(range(len(lrs)))\n",
    "ax4.set_xticklabels(lrs)\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Decision Boundary Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary (using first two features)\n",
    "def plot_decision_boundary(X, y, perceptron, feature_names):\n",
    "    # Create a mesh\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Make predictions on mesh (using average yellow_score)\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel(), \n",
    "                       np.full(xx.ravel().shape, X[:, 2].mean())]\n",
    "    Z = perceptron.predict_proba(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "    plt.colorbar(label='Prediction Probability')\n",
    "    \n",
    "    # Plot data points\n",
    "    colors = ['red', 'yellow']\n",
    "    labels = ['Apple', 'Banana']\n",
    "    for i in [0, 1]:\n",
    "        plt.scatter(X[y == i, 0], X[y == i, 1], \n",
    "                   c=colors[i], label=labels[i], \n",
    "                   edgecolors='black', s=100, alpha=0.9)\n",
    "    \n",
    "    plt.xlabel(feature_names[0])\n",
    "    plt.ylabel(feature_names[1])\n",
    "    plt.title('Decision Boundary (Yellow Score = Average)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary(X_normalized, y, perceptron, \n",
    "                      ['Length (normalized)', 'Weight (normalized)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== REFLECTION ANALYSIS ===\")\n",
    "print(\"\\n1. Initial Random Predictions vs Final Results:\")\n",
    "print(f\"   - Initial accuracy: {results['initial_accuracy']:.4f} ({results['initial_accuracy']*100:.1f}%)\")\n",
    "print(f\"   - Final accuracy: {results['final_accuracy']:.4f} ({results['final_accuracy']*100:.1f}%)\")\n",
    "print(f\"   - Improvement: {(results['final_accuracy'] - results['initial_accuracy'])*100:.1f} percentage points\")\n",
    "print(f\"   - Initial loss: {results['initial_cost']:.4f}\")\n",
    "print(f\"   - Final loss: {results['final_cost']:.4f}\")\n",
    "\n",
    "print(\"\\n2. Learning Rate Impact on Convergence:\")\n",
    "for lr in learning_rates:\n",
    "    res = results_comparison[lr]\n",
    "    print(f\"   - LR {lr:5.3f}: {res['epochs_trained']:3d} epochs, \"\n",
    "          f\"accuracy {res['final_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n3. DJ-Knob / Child-Learning Analogy:\")\n",
    "print(\"   The learning rate acts like a 'DJ knob' controlling how quickly\")\n",
    "print(\"   our perceptron learns from mistakes:\")\n",
    "print(f\"   - Too low (0.001): Like a timid child, learns very slowly\")\n",
    "print(f\"   - Just right (0.01-0.1): Like an attentive student, steady progress\")\n",
    "print(f\"   - Too high (1.0): Like an impulsive child, may overshoot the target\")\n",
    "\n",
    "# Show the learning curves for different LRs side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Focus on first 100 epochs to see differences\n",
    "for i, lr in enumerate([0.001, 0.01, 0.1, 1.0]):\n",
    "    perceptron_viz = Perceptron(learning_rate=lr, random_seed=42)\n",
    "    perceptron_viz.fit(X_normalized, y, epochs=100, verbose=False)\n",
    "    \n",
    "    ax1.plot(perceptron_viz.cost_history[:50], \n",
    "             color=colors[i], label=f'LR={lr}', linewidth=2, marker='o', markersize=3)\n",
    "    ax2.plot(perceptron_viz.accuracy_history[:50], \n",
    "             color=colors[i], label=f'LR={lr}', linewidth=2, marker='s', markersize=3)\n",
    "\n",
    "ax1.set_title('Learning Speed Comparison (First 50 Epochs)', fontsize=14)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_title('Accuracy Improvement (First 50 Epochs)', fontsize=14)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Dataset Creation**: 16 fruit samples with length, weight, and yellowness features\n",
    "2. **Pure NumPy Implementation**: Single-neuron logistic regression from scratch\n",
    "3. **Batch Gradient Descent**: Training with 500+ epochs until loss < 0.05\n",
    "4. **Visualization**: Training metrics and decision boundary plots\n",
    "5. **Learning Rate Analysis**: Impact on convergence speed and stability\n",
    "6. **Child Learning Analogy**: How learning rate affects the learning process\n",
    "\n",
    "The perceptron successfully learned to distinguish apples from bananas using their physical characteristics!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}